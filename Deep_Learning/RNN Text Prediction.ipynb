{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is to use recurrent neural networks, LSTMs, GRUs and Pytorch sequence-to-sequence capabilities for text prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import pdb\n",
    " \n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    " \n",
    "file = unidecode.unidecode(open('./text_files/tiny_shakespeare.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " so she yields to me;\n",
      "For I am rough and woo not like a babe.\n",
      "\n",
      "BAPTISTA:\n",
      "Well mayst thou woo, and happy be thy speed!\n",
      "But be thou arm'd for some unhappy words.\n",
      "\n",
      "PETRUCHIO:\n",
      "Ay, to the proof; as mountain\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "#Code for getting a random chunk from a specific text\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    " \n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    " \n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a random set of a larger text corpus to train on\n",
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    ## initialize hidden variable, initialize other useful variables \n",
    "    predicted = prime_str\n",
    "    hidden = decoder.init_hidden()  \n",
    "    ## /\n",
    " \n",
    "    prime_input = char_tensor(prime_str)\n",
    " \n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    " \n",
    "    for c in range(predict_len):\n",
    "        output, hidden =  decoder(inp, hidden) \n",
    " \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    " \n",
    "        ## get character from your list of all characters, add it to your output str sequence, set input\n",
    "        ## for the next pass through the model\n",
    "        letter=all_characters[top_i.item()]\n",
    "        predicted += letter\n",
    "        inp= top_i\n",
    " \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.037445306777954 (100 5%) 2.5389]\n",
      "Whit in y oth\n",
      " thoud noad nour the heast men the thisal Hof dond Eifess, darthen is cbertheseseses you \n",
      "\n",
      "[22.40361452102661 (200 10%) 2.1421]\n",
      "Wh four in wour hee nougler?\n",
      "Foort I dint so; you put soucs as het net forpis the ame sing; ous my thi \n",
      "\n",
      "[34.800031423568726 (300 15%) 2.0377]\n",
      "Whe the\n",
      "To thnouss stened, to cil' ond and movere.\n",
      "un Ment in susle dat the the leangese tued morires  \n",
      "\n",
      "[49.21166801452637 (400 20%) 2.0454]\n",
      "Whave to Cord well conos, is thou the thent\n",
      "And thent ear my so there and shie call felld semut wich t \n",
      "\n",
      "[63.78067445755005 (500 25%) 1.9059]\n",
      "Whe to so me to but the a neet?\n",
      "\n",
      "ORIANIES:\n",
      "Mace for lore my fards I dave spire sey!\n",
      "\n",
      "LUMETSE:\n",
      "Ke has s \n",
      "\n",
      "[79.25118589401245 (600 30%) 2.0035]\n",
      "Whe in his ners is your me and that in as still he rearringardy.\n",
      "\n",
      "\n",
      "CHAROLO:\n",
      "Riot him recas in hour he  \n",
      "\n",
      "[93.5791826248169 (700 35%) 1.8882]\n",
      "What greand of the so ever haugrey\n",
      "What wo dreas me degreast be there yee chood go not my goress, shmo \n",
      "\n",
      "[108.34155011177063 (800 40%) 2.0124]\n",
      "Where, the read.\n",
      "\n",
      "QUENTESIUS:\n",
      "Eds, do come fill whe not mu?\n",
      "Hear canst see tere tray oft of the reanfo \n",
      "\n",
      "[123.12339925765991 (900 45%) 1.9052]\n",
      "Where, hast a flear ull? feer's, and Ceep the prilo did:\n",
      "Ewor, wall in the rothar, nose with of shour  \n",
      "\n",
      "[138.54118394851685 (1000 50%) 2.0065]\n",
      "Why gattes gost my lord the some.\n",
      "\n",
      "SARD I ESSARD I:\n",
      "That se that it of that ay, That your cing your wo \n",
      "\n",
      "[153.81710147857666 (1100 55%) 1.8202]\n",
      "Whess on whris: fath,\n",
      "Where enqued your great and rush your to man kings hat wither here,\n",
      "And well sol \n",
      "\n",
      "[170.94051003456116 (1200 60%) 1.9501]\n",
      "Whis come and poot more turder\n",
      "Exh ay in a light here prand, is wourd.\n",
      "\n",
      "HORCENTEO:\n",
      "But and heave I dor \n",
      "\n",
      "[186.27737498283386 (1300 65%) 1.9752]\n",
      "What neminger,\n",
      "Furous to my and thy my tour the ny thee ourth so forther made\n",
      "He maln, burbent, my loo \n",
      "\n",
      "[202.20242285728455 (1400 70%) 1.8837]\n",
      "When the have hasel!\n",
      "\n",
      "fold and seest but amales of the day:\n",
      "And hath the adise ast of and thou nes day \n",
      "\n",
      "[216.85797262191772 (1500 75%) 2.1808]\n",
      "Which the.\n",
      "\n",
      "CORIARTIRCK:\n",
      "I shall be thou give there:\n",
      "Whondord the lorster come thy nath the for me she \n",
      "\n",
      "[231.54277396202087 (1600 80%) 1.8645]\n",
      "Who the my for I wadks\n",
      "Ht then wo danguels sird, them bear theeself\n",
      "Sen his the mean that becech tuke  \n",
      "\n",
      "[248.35428857803345 (1700 85%) 1.6557]\n",
      "Which disle thou rable and they soned, not life.\n",
      "\n",
      "HERCHARD II:\n",
      "Your theridest bickinty in him the blee \n",
      "\n",
      "[262.3154354095459 (1800 90%) 1.8776]\n",
      "While with the sorabon:\n",
      "He come the cuntht make:\n",
      "Are a cimion, thou this being your mear and elder.\n",
      "\n",
      "P \n",
      "\n",
      "[276.7273724079132 (1900 95%) 1.9559]\n",
      "Why pued,\n",
      "If sige of them well shall conmelded a dear, and comeged the gals,\n",
      "I saw shall well mence th \n",
      "\n",
      "[291.42462730407715 (2000 100%) 1.9311]\n",
      "Which have the knoth be where with so.\n",
      "\n",
      "SAUMINA:\n",
      "Bill to my swer,\n",
      "What her deet their mike the besent  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    " \n",
    "#The RNN model \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        # encode using embedding layer\n",
    "        # set up GRU passing in number of layers parameter (nn.GRU)\n",
    "        # decode output\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    " \n",
    "    def forward(self, input_char, hidden):\n",
    "        output = self.embedding(input_char).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "        \n",
    "        # by reviewing the documentation, construct a forward function that properly uses the output\n",
    "        # of the GRU\n",
    "        # return output and hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "    \n",
    "        \n",
    "def train(inp, target):\n",
    "    decoder_optimizer.zero_grad()\n",
    "    hidden = decoder.init_hidden()       \n",
    "\n",
    "    # initialize hidden layers, set up gradient and loss\n",
    "    loss = 0\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)# run the forward pass of your rnn with proper input\n",
    "        loss += criterion(output, target[c].unsqueeze(0))\n",
    " \n",
    "    # calculate backwards loss and step the optimizer (globaly)\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()          \n",
    "    return loss.item() / chunk_len\n",
    "    \n",
    "    \n",
    "import time\n",
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.005\n",
    " \n",
    "#Make the decoder, its optimizer, and loss function\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    " \n",
    "#Run the epoch for the amount you want to do\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss_ = train(*random_training_set())       \n",
    "    loss_avg += loss_\n",
    "     \n",
    "    #See what our text predictor predicts every 100 epochs\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "     \n",
    "    #Get the average loss\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
