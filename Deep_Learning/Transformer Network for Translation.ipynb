{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the transformer model, which is a translator from spanish to english, from \"Attention is All You Need.\" https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import pdb\n",
    "import pandas\n",
    " \n",
    "datas = pandas.read_csv( 'en-es_conference.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "                        \n",
    "        model_opt.step()\n",
    "        model_opt.optimizer.zero_grad()\n",
    "        if i % 10 == 1:\n",
    "            print(i, loss, model_opt._rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "\n",
    "import spacy\n",
    "spacy_sp = spacy.load('es')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_sp(text):\n",
    "    return [tok.text for tok in spacy_sp.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "\n",
    "SRC = data.Field(tokenize=tokenize_sp)\n",
    "TGT = data.Field(tokenize=tokenize_en)\n",
    "\n",
    "MAX_LEN = 30\n",
    "MIN_LEN = 10\n",
    "tab = data.TabularDataset(\n",
    "    path='en-es_conference.csv',format = 'csv', skip_header=True, fields=[('id',None),('trg',TGT),('src',SRC)], \n",
    "    filter_pred=lambda x: ((len(vars(x)['src']) <= MAX_LEN) and \n",
    "        (len(vars(x)['trg']) <= MAX_LEN)) and (len(vars(x)['src']) >= MIN_LEN) and (len(vars(x)['trg']) >= MIN_LEN))\n",
    "train, val = tab.split()\n",
    "MIN_FREQ = 2\n",
    "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    return Batch(src, trg, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_backprop(generator, criterion, out, targets, normalize):\n",
    "    \"\"\"\n",
    "    Memory optmization. Compute each timestep separately and sum grads.\n",
    "    \"\"\"\n",
    "    assert out.size(1) == targets.size(1)\n",
    "    total = 0.0\n",
    "    out_grad = []\n",
    "    for i in range(out.size(1)):\n",
    "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
    "        gen = generator(out_column)\n",
    "        loss = criterion(gen, targets[:, i]) / normalize\n",
    "        total += loss.data[0]\n",
    "        loss.backward()\n",
    "        out_grad.append(out_column.grad.data.clone())\n",
    "    out_grad = torch.stack(out_grad, dim=1)\n",
    "    out.backward(gradient=out_grad)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
    "    model.test()\n",
    "    total = 0\n",
    "    for batch in valid_iter:\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "model.cuda()\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()\n",
    "BATCH_SIZE = 200\n",
    "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True)\n",
    "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(16128, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(12089, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=12089)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "model_opt = get_std_opt(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7.727381408214569 6.987712429686844e-07\n",
      "11 7.414251148700714 4.192627457812107e-06\n",
      "21 7.489433467388153 7.686483672655528e-06\n",
      "31 7.3935337364673615 1.118033988749895e-05\n",
      "41 7.244868874549866 1.4674196102342371e-05\n",
      "51 7.503334939479828 1.8168052317185794e-05\n",
      "61 7.089837461709976 2.1661908532029216e-05\n",
      "71 6.98610183596611 2.515576474687264e-05\n",
      "81 6.847178608179092 2.8649620961716057e-05\n",
      "91 6.788153797388077 3.214347717655948e-05\n",
      "101 6.595185071229935 3.56373333914029e-05\n",
      "111 6.410266190767288 3.913118960624633e-05\n",
      "121 6.068511813879013 4.262504582108975e-05\n",
      "131 6.3478134870529175 4.611890203593317e-05\n",
      "141 6.25893098115921 4.961275825077659e-05\n",
      "151 5.999349921941757 5.310661446562001e-05\n",
      "161 6.201990902423859 5.660047068046343e-05\n",
      "171 5.815389633178711 6.0094326895306855e-05\n",
      "181 5.579428106546402 6.358818311015028e-05\n",
      "191 5.28668025135994 6.70820393249937e-05\n",
      "201 5.7671027183532715 7.057589553983712e-05\n",
      "211 5.548916220664978 7.406975175468054e-05\n",
      "221 5.548367381095886 7.756360796952397e-05\n",
      "231 5.57673653960228 8.10574641843674e-05\n",
      "241 5.315808117389679 8.455132039921081e-05\n",
      "251 5.51480758190155 8.804517661405423e-05\n",
      "261 5.470993489027023 9.153903282889765e-05\n",
      "271 5.3061259388923645 9.503288904374107e-05\n",
      "281 5.463704004883766 9.85267452585845e-05\n",
      "291 5.447747617959976 0.00010202060147342792\n",
      "301 5.246341288089752 0.00010551445768827134\n",
      "311 5.352418124675751 0.00010900831390311476\n",
      "321 5.639787554740906 0.00011250217011795819\n",
      "331 5.783810243010521 0.0001159960263328016\n",
      "341 5.261634737253189 0.00011948988254764501\n",
      "351 5.480943277478218 0.00012298373876248845\n",
      "361 5.4138153195381165 0.00012647759497733186\n",
      "371 5.277960017323494 0.0001299714511921753\n",
      "381 5.505034506320953 0.00013346530740701871\n",
      "391 5.677164793014526 0.00013695916362186213\n",
      "401 5.554487764835358 0.00014045301983670557\n",
      "411 5.9455841183662415 0.00014394687605154898\n",
      "421 5.11091411113739 0.00014744073226639242\n",
      "431 5.6015340983867645 0.00015093458848123583\n",
      "441 5.210084050893784 0.00015442844469607924\n",
      "451 5.1052776873111725 0.00015792230091092268\n",
      "461 4.869116246700287 0.0001614161571257661\n",
      "471 5.416734263300896 0.0001649100133406095\n",
      "481 5.441170513629913 0.00016840386955545295\n",
      "491 5.2026591300964355 0.00017189772577029636\n",
      "501 4.966461479663849 0.00017539158198513977\n",
      "511 5.4149224907159805 0.0001788854381999832\n",
      "521 5.193384110927582 0.00018237929441482662\n",
      "531 5.400321871042252 0.00018587315062967003\n",
      "541 5.115745335817337 0.00018936700684451347\n",
      "551 5.399890914559364 0.00019286086305935688\n",
      "561 5.058987259864807 0.0001963547192742003\n",
      "571 4.542431592941284 0.00019984857548904374\n",
      "581 5.421290263533592 0.00020334243170388715\n",
      "591 5.322878047823906 0.0002068362879187306\n",
      "601 4.954870566725731 0.000210330144133574\n",
      "611 4.809737280011177 0.0002138240003484174\n",
      "621 5.21947306394577 0.00021731785656326085\n",
      "631 5.36196568608284 0.00022081171277810424\n",
      "641 4.77370847761631 0.00022430556899294768\n",
      "651 4.700960159301758 0.00022779942520779112\n",
      "661 4.773119479417801 0.0002312932814226345\n",
      "671 4.960130155086517 0.00023478713763747794\n",
      "681 5.161930292844772 0.00023828099385232138\n",
      "691 4.684029534459114 0.00024177485006716476\n",
      "701 4.939477577805519 0.00024526870628200823\n",
      "711 4.561660721898079 0.00024876256249685164\n",
      "721 4.877705559134483 0.00025225641871169505\n",
      "731 4.815193884074688 0.00025575027492653847\n",
      "741 4.530046880245209 0.0002592441311413819\n",
      "751 4.92424102127552 0.00026273798735622535\n",
      "761 4.663077667355537 0.00026623184357106876\n",
      "771 4.95988230407238 0.00026972569978591217\n",
      "781 4.920532613992691 0.0002732195560007556\n",
      "791 4.832117632031441 0.000276713412215599\n",
      "801 5.262487381696701 0.0002802072684304424\n",
      "811 4.619173943996429 0.0002837011246452859\n",
      "821 4.806833207607269 0.0002871949808601293\n",
      "831 4.973114922642708 0.0002906888370749727\n",
      "841 5.087825909256935 0.0002941826932898161\n",
      "851 4.790994510054588 0.0002976765495046596\n",
      "861 4.66692154109478 0.00030117040571950293\n",
      "871 4.831724137067795 0.0003046642619343464\n",
      "881 4.942073225975037 0.0003081581181491898\n",
      "891 5.114459425210953 0.0003116519743640332\n",
      "901 4.817439213395119 0.00031514583057887664\n",
      "911 4.478170000016689 0.0003186396867937201\n",
      "921 4.687752276659012 0.00032213354300856346\n",
      "931 4.762510076165199 0.00032562739922340693\n",
      "941 4.771345794200897 0.00032912125543825034\n",
      "951 5.007248610258102 0.00033261511165309375\n",
      "961 5.083863839507103 0.00033610896786793716\n",
      "971 4.422773867845535 0.00033960282408278063\n",
      "981 4.277925595641136 0.000343096680297624\n",
      "991 5.09337592124939 0.00034659053651246746\n",
      "1001 5.139251887798309 0.0003500843927273108\n",
      "1011 4.899629771709442 0.00035357824894215433\n",
      "1021 4.49843093752861 0.0003570721051569977\n",
      "1031 4.81746369600296 0.0003605659613718411\n",
      "1041 4.8760973662137985 0.00036405981758668457\n",
      "1051 4.763363093137741 0.000367553673801528\n",
      "1061 4.341105900704861 0.00037104753001637145\n",
      "1071 4.734584040939808 0.0003745413862312148\n",
      "1081 4.515105947852135 0.0003780352424460582\n",
      "1091 4.678268730640411 0.0003815290986609017\n",
      "1101 4.587428882718086 0.0003850229548757451\n",
      "1111 4.483395338058472 0.0003885168110905885\n",
      "1121 4.129518136382103 0.000392010667305432\n",
      "1131 4.110916905105114 0.00039550452352027533\n",
      "1141 5.1146135404706 0.00039899837973511875\n",
      "1151 4.45775043964386 0.0004024922359499622\n",
      "1161 5.1475377678871155 0.0004059860921648056\n",
      "1171 4.2800620794296265 0.00040947994837964904\n",
      "1181 4.434310272336006 0.0004129738045944925\n",
      "1191 4.765246652066708 0.00041646766080933586\n",
      "1201 5.049677103757858 0.0004199615170241793\n",
      "1211 4.3864657655358315 0.00042345537323902274\n",
      "1221 4.117211699485779 0.00042694922945386615\n",
      "1231 4.504618912935257 0.00043044308566870957\n",
      "1241 4.55303680896759 0.00043393694188355303\n",
      "1251 4.212335020303726 0.0004374307980983964\n",
      "1261 4.8573566526174545 0.0004409246543132398\n",
      "1271 4.238170795142651 0.00044441851052808327\n",
      "1281 4.7611474096775055 0.0004479123667429267\n",
      "1291 4.510849758982658 0.0004514062229577701\n",
      "1301 4.5023980140686035 0.00045490007917261356\n",
      "1311 4.543791837990284 0.0004583939353874569\n",
      "1321 5.116308391094208 0.0004618877916023004\n",
      "1331 4.3603143990039825 0.0004653816478171438\n",
      "1341 4.302772410213947 0.0004688755040319872\n",
      "1351 4.253482401371002 0.0004723693602468307\n",
      "1361 4.817860126495361 0.0004758632164616741\n",
      "1371 4.292443856596947 0.00047935707267651744\n",
      "1381 4.441678516566753 0.0004828509288913609\n",
      "1391 4.513461410999298 0.0004863447851062043\n",
      "1401 4.840218767523766 0.0004898386413210477\n",
      "1411 4.396689400076866 0.0004933324975358911\n",
      "1421 5.380453512072563 0.0004968263537507346\n",
      "1431 5.011884093284607 0.000500320209965578\n",
      "1441 4.576200135052204 0.0005038140661804215\n",
      "1451 4.393270671367645 0.0005073079223952649\n",
      "1461 4.380741402506828 0.0005108017786101082\n",
      "1471 4.079135984182358 0.0005142956348249517\n",
      "1481 4.544367156922817 0.0005177894910397951\n",
      "1491 4.801519438624382 0.0005212833472546386\n",
      "1501 4.682016432285309 0.000524777203469482\n",
      "1511 4.442531660199165 0.0005282710596843254\n",
      "1521 4.536256447434425 0.0005317649158991688\n",
      "1531 4.545325830578804 0.0005352587721140122\n",
      "1541 4.127439349889755 0.0005387526283288556\n",
      "1551 4.429707467556 0.000542246484543699\n",
      "1561 4.496988773345947 0.0005457403407585425\n",
      "1571 5.04242268204689 0.0005492341969733859\n",
      "1581 4.509887859225273 0.0005527280531882293\n",
      "1591 5.32367542386055 0.0005562219094030728\n",
      "1601 4.88132917881012 0.0005597157656179162\n",
      "1611 4.940099388360977 0.0005632096218327596\n",
      "1621 4.607124298810959 0.000566703478047603\n",
      "1631 4.382220417261124 0.0005701973342624464\n",
      "1641 4.881800651550293 0.00057369119047729\n",
      "1651 4.192868962883949 0.0005771850466921333\n",
      "1661 4.135174944996834 0.0005806789029069767\n",
      "1671 4.471620202064514 0.0005841727591218202\n",
      "1681 4.257706128060818 0.0005876666153366636\n",
      "1691 4.9402134865522385 0.0005911604715515069\n",
      "1701 4.454280182719231 0.0005946543277663504\n",
      "1711 4.131395012140274 0.0005981481839811938\n",
      "1721 4.271519303321838 0.0006016420401960373\n",
      "1731 4.13521359115839 0.0006051358964108807\n",
      "1741 4.271172419190407 0.0006086297526257241\n",
      "1751 4.348846226930618 0.0006121236088405675\n",
      "1761 4.288166582584381 0.0006156174650554109\n",
      "1771 4.184117287397385 0.0006191113212702543\n",
      "1781 4.335064172744751 0.0006226051774850977\n",
      "1791 4.285342633724213 0.0006260990336999412\n",
      "1801 4.923044040799141 0.0006295928899147847\n",
      "1811 4.1704496666789055 0.000633086746129628\n",
      "1821 4.500933900475502 0.0006365806023444715\n",
      "1831 4.421135284006596 0.0006400744585593149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1841 4.400330267846584 0.0006435683147741583\n",
      "1851 4.448333263397217 0.0006470621709890017\n",
      "1861 4.633141316473484 0.0006505560272038451\n",
      "1871 4.2306927144527435 0.0006540498834186887\n",
      "1881 4.48474195599556 0.000657543739633532\n",
      "1891 4.364267528057098 0.0006610375958483754\n",
      "1901 4.7622451186180115 0.0006645314520632189\n",
      "1911 4.4212073385715485 0.0006680253082780623\n",
      "1921 3.738890126347542 0.0006715191644929057\n",
      "1931 4.749606013298035 0.0006750130207077491\n",
      "1941 4.241527549922466 0.0006785068769225925\n",
      "1951 4.151103541254997 0.000682000733137436\n",
      "1961 4.366244234144688 0.0006854945893522794\n",
      "1971 4.471244722604752 0.0006889884455671228\n",
      "1981 4.534580051898956 0.0006924823017819663\n",
      "1991 4.282895937561989 0.0006959761579968097\n",
      "2001 4.710340321063995 0.000699470014211653\n",
      "2011 4.078686818480492 0.0007029638704264964\n",
      "2021 4.162652805447578 0.0007064577266413398\n",
      "2031 4.389845564961433 0.0007099515828561834\n",
      "2041 4.374806113541126 0.0007134454390710268\n",
      "2051 3.5975100994110107 0.0007169392952858702\n",
      "2061 4.318098925054073 0.0007204331515007136\n",
      "2071 5.034056976437569 0.000723927007715557\n",
      "2081 4.238338842988014 0.0007274208639304004\n",
      "2091 4.54386992752552 0.0007309147201452439\n",
      "2101 4.02679917216301 0.0007344085763600873\n",
      "2111 2.9379231249913573 0.0007379024325749308\n",
      "2121 4.577775523066521 0.0007413962887897741\n",
      "2131 4.541389510035515 0.0007448901450046175\n",
      "2141 4.562172576785088 0.0007483840012194609\n",
      "2151 5.012739688158035 0.0007518778574343044\n",
      "2161 4.396959990262985 0.0007553717136491478\n",
      "2171 4.514430865645409 0.0007588655698639912\n",
      "2181 4.255661904811859 0.0007623594260788346\n",
      "2191 4.273117765784264 0.0007658532822936781\n",
      "2201 3.969868987798691 0.0007693471385085215\n",
      "2211 4.049687296152115 0.000772840994723365\n",
      "2221 3.8290534615516663 0.0007763348509382084\n",
      "2231 4.69224214553833 0.0007798287071530518\n",
      "2241 4.4450195878744125 0.0007833225633678951\n",
      "2251 4.71507816016674 0.0007868164195827385\n",
      "2261 4.7502173483371735 0.0007903102757975819\n",
      "2271 4.5879355147480965 0.0007938041320124255\n",
      "2281 4.776229918003082 0.0007972979882272689\n",
      "2291 4.313528001308441 0.0008007918444421123\n",
      "2301 4.366696193814278 0.0008042857006569557\n",
      "2311 4.000548616051674 0.0008077795568717991\n",
      "2321 4.258540719747543 0.0008112734130866426\n",
      "2331 3.9515131413936615 0.000814767269301486\n",
      "2341 4.12146782130003 0.0008182611255163295\n",
      "2351 4.877397060394287 0.0008217549817311728\n",
      "2361 3.8980106115341187 0.0008252488379460162\n",
      "2371 4.737999148666859 0.0008287426941608596\n",
      "2381 4.289362892508507 0.0008322365503757031\n",
      "2391 4.524705223739147 0.0008357304065905465\n",
      "2401 3.9649081975221634 0.0008392242628053899\n",
      "2411 4.144341990351677 0.0008427181190202333\n",
      "2421 4.022219605743885 0.0008462119752350768\n",
      "2431 4.100216783583164 0.0008497058314499202\n",
      "2441 4.625527158379555 0.0008531996876647637\n",
      "2451 4.57283254340291 0.0008566935438796071\n",
      "2461 4.5732030645012856 0.0008601874000944505\n",
      "2471 4.243740163743496 0.0008636812563092938\n",
      "2481 3.140856008976698 0.0008671751125241372\n",
      "2491 4.567759156227112 0.0008706689687389806\n",
      "2501 4.202290251851082 0.0008741628249538242\n",
      "2511 4.833281427621841 0.0008776566811686676\n",
      "2521 4.321711108088493 0.000881150537383511\n",
      "2531 3.764846056699753 0.0008846443935983544\n",
      "2541 4.912494659423828 0.0008881382498131978\n",
      "2551 4.785814017057419 0.0008916321060280413\n",
      "2561 5.006949543952942 0.0008951259622428847\n",
      "2571 4.476939961314201 0.0008986198184577282\n",
      "2581 4.594588242471218 0.0009021136746725716\n",
      "2591 4.135493878275156 0.0009056075308874149\n",
      "2601 4.321453675627708 0.0009091013871022583\n",
      "2611 4.534931614995003 0.0009125952433171018\n",
      "2621 3.9747723191976547 0.0009160890995319452\n",
      "2631 4.939613610506058 0.0009195829557467886\n",
      "2641 4.673055365681648 0.000923076811961632\n",
      "2651 3.7748896181583405 0.0009265706681764755\n",
      "2661 4.1997991502285 0.0009300645243913189\n",
      "2671 4.376220986247063 0.0009335583806061624\n",
      "2681 4.032203391194344 0.0009370522368210058\n",
      "2691 4.304295301437378 0.0009405460930358492\n",
      "2701 4.795854434370995 0.0009440399492506926\n",
      "2711 4.397275343537331 0.0009475338054655359\n",
      "2721 4.218376517295837 0.0009510276616803793\n",
      "2731 4.491505704820156 0.0009545215178952229\n",
      "2741 4.234985113143921 0.0009580153741100663\n",
      "2751 5.190770849585533 0.0009615092303249097\n",
      "2761 4.385032773017883 0.0009650030865397531\n",
      "2771 4.061621129512787 0.0009684969427545965\n",
      "2781 4.38286679983139 0.0009719907989694399\n",
      "2791 4.486547343432903 0.0009754846551842834\n",
      "2801 4.052266091108322 0.0009789785113991267\n",
      "2811 4.3076967895030975 0.0009824723676139702\n",
      "2821 4.931750237941742 0.0009859662238288136\n",
      "2831 4.272637352347374 0.000989460080043657\n",
      "2841 4.54229861497879 0.0009929539362585006\n",
      "2851 4.13499253988266 0.000996447792473344\n",
      "2861 4.665714770555496 0.0009999416486881874\n",
      "2871 3.8118709102272987 0.0010034355049030308\n",
      "2881 4.479560405015945 0.001006929361117874\n",
      "2891 4.4274371564388275 0.0010104232173327175\n",
      "2901 4.581253379583359 0.001013917073547561\n",
      "2911 4.127282410860062 0.0010174109297624045\n",
      "2921 4.788213387131691 0.001020904785977248\n",
      "2931 4.709438696503639 0.0010243986421920913\n",
      "2941 3.9344112128019333 0.0010278924984069347\n",
      "2951 4.104755207896233 0.0010313863546217781\n",
      "2961 4.146623849868774 0.0010348802108366216\n",
      "2971 4.626170217990875 0.001038374067051465\n",
      "2981 4.961372792720795 0.0010418679232663084\n",
      "2991 4.51252193748951 0.0010453617794811518\n",
      "3001 4.122734971344471 0.0010488556356959952\n",
      "3011 4.578122779726982 0.0010523494919108386\n",
      "3021 4.265448868274689 0.0010558433481256822\n",
      "3031 3.727904826402664 0.0010593372043405254\n",
      "3041 3.751543015241623 0.0010628310605553689\n",
      "3051 4.613467417657375 0.0010663249167702123\n",
      "3061 4.318317398428917 0.0010698187729850557\n",
      "3071 4.674579545855522 0.0010733126291998993\n",
      "3081 4.6574617102742195 0.0010768064854147427\n",
      "3091 4.874323345720768 0.0010803003416295861\n",
      "3101 5.335960432887077 0.0010837941978444295\n",
      "3111 4.107190191745758 0.001087288054059273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-4169ed0058ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrebatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m#  valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1735085d52f5>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_iter, model, criterion, opt, transpose)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmodel_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-cac760f2f089>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(5):\n",
    "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)\n",
    "  #  valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation:\tand our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our Father and our \n",
      "Target:\thears and answers the prayers of His children . \n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    for i, batch in enumerate(valid_iter):\n",
    "        src = batch.src.transpose(0, 1)[:1]\n",
    "        src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
    "        out = greedy_decode(model, src, src_mask, \n",
    "                            max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
    "        print(\"Translation:\", end=\"\\t\")\n",
    "        for i in range(1, out.size(1)):\n",
    "            sym = TGT.vocab.itos[out[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            print(sym, end =\" \")\n",
    "        print()\n",
    "        print(\"Target:\", end=\"\\t\")\n",
    "        for i in range(1, batch.trg.size(0)):\n",
    "            sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
    "            if sym == \"</s>\": break\n",
    "            print(sym, end =\" \")\n",
    "        print()\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
