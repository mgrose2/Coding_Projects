{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "Modify `scrape_books()` so that it gathers the price for each fiction book and\n",
    "returns the mean price, in Â£, of a fiction book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_books(start_page = \"index.html\"):\n",
    "    \"\"\" Crawl through http://books.toscrape.com and extract fiction data\"\"\"\n",
    "    base_url=\"http://books.toscrape.com/catalogue/category/books/fiction_10/\"\n",
    "    prices = []\n",
    "    page = base_url + start_page                # Complete page URL.\n",
    "    next_page_finder = re.compile(r\"next\")      # We need this button.\n",
    "    \n",
    "    current = None\n",
    "\n",
    "    for _ in range(4):\n",
    "        while current == None:                   # Try downloading until it works.\n",
    "            # Download the page source and PAUSE before continuing.  \n",
    "            page_source = requests.get(page).text\n",
    "            time.sleep(1)           # PAUSE before continuing.\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            current = soup.find_all(class_=\"price_color\")\n",
    "    \n",
    "            \n",
    "        # Navigate to the correct tag and extract title.\n",
    "        for book in current:\n",
    "            prices.append(float(book.string[2:]))\n",
    "    \n",
    "        # ind the URL for the page with the next data\n",
    "        if \"page-4\" not in page:\n",
    "            # Find the URL for the page with the next data.\n",
    "            new_page = soup.find(string=next_page_finder).parent[\"href\"]    \n",
    "            page = base_url + new_page      # New complete page URL.\n",
    "            current = None\n",
    "    return float(np.mean(prices))\n",
    "    return NotImplementedError(\"Problem 1 Incomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the value as a pickle file\n",
    "value = scrape_books()\n",
    "with open(\"ans1\", \"wb\") as fp:\n",
    "    pickle.dump(value,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Modify `bank_data()` so that it extracts the total consolidated assets (\"Consol\n",
    "Assets\") for JPMorgan Chase, Bank of America, and Wells Fargo recorded each December from\n",
    "2004 to the present. Return a list of lists where each list contains the assets of each bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bank_data():\n",
    "    \"\"\"Crawl through the Federal Reserve site and extract bank data.\"\"\"\n",
    "    # Compile regular expressions for finding certain tags.\n",
    "    link_finder = re.compile(r\"December 31, 20(0[4-9]|1[0-9])\")\n",
    "    chase_bank_finder = re.compile(r\"^JPMORGAN CHASE BK\")\n",
    "    america_bank_finder = re.compile(r\"^BANK OF AMER\")\n",
    "    wells_fargo_finder = re.compile(r\"^WELLS FARGO BK\")\n",
    "\n",
    "    # Get the base page and find the URLs to all other relevant pages.\n",
    "    base_url=\"https://www.federalreserve.gov/releases/lbr/\"\n",
    "    base_page_source = requests.get(base_url).text\n",
    "    base_soup = BeautifulSoup(base_page_source, \"html.parser\")\n",
    "    link_tags = base_soup.find_all(name='a', href=True, string=link_finder)\n",
    "    pages = [base_url + tag.attrs[\"href\"] for tag in link_tags]\n",
    "    \n",
    "    # Crawl through the individual pages and record the data.\n",
    "    chase_assets = []\n",
    "    america_assets = []\n",
    "    wells_assets = []\n",
    "    for page in pages:\n",
    "        time.sleep(1)               # PAUSE, then request the page.\n",
    "        soup = BeautifulSoup(requests.get(page).text, \"html.parser\")\n",
    "\n",
    "        # Find the tag corresponding to the banks' consolidated assets.\n",
    "        chase_temp_tag = soup.find(name=\"td\", string=chase_bank_finder)\n",
    "        america_temp_tag = soup.find(name=\"td\", string=america_bank_finder)\n",
    "        wells_temp_tag = soup.find(name=\"td\", string=wells_fargo_finder)\n",
    "\n",
    "        for _ in range(10):\n",
    "            chase_temp_tag = chase_temp_tag.next_sibling\n",
    "            america_temp_tag = america_temp_tag.next_sibling\n",
    "            wells_temp_tag = wells_temp_tag.next_sibling  \n",
    "        # Extract the data, removing commas.\n",
    "        chase_assets.append(int(chase_temp_tag.string.replace(',', '')))\n",
    "        america_assets.append(int(america_temp_tag.string.replace(',', '')))\n",
    "        wells_assets.append(int(wells_temp_tag.string.replace(',', '')))\n",
    "\n",
    "    return([chase_assets, america_assets, wells_assets])\n",
    "    raise NotImplementedError(\"Problem 4 Incomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the list of lists as a pickle file\n",
    "value = bank_data()\n",
    "with open(\"ans2\", \"wb\") as fp:\n",
    "    pickle.dump(value,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "The Basketball Reference website at `https://www.basketball-reference.com`\n",
    "contains data on NBA athletes, including which player led different categories for each season.\n",
    "For the past ten seasons, identify which player had the most season points and find how many\n",
    "points they scored during that season. Return a list of triples consisting of the season, the\n",
    "player, and the points scored, (\"season year\", \"player name\", points scored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob3():\n",
    "    '''The Basketball Reference website at \n",
    "    https://www.basketball-reference.com} hosts data on NBA athletes, \n",
    "    including which player led different categories.\n",
    "    For the past ten years, identify which player had the most season points.\n",
    "    Return a list of triples, (\"season year\", \"player name\", points scored).\n",
    "    '''\n",
    "    # Compile regular expressions for finding certain tags.\n",
    "    link_finder = re.compile(r\"1[0-9] Leaders$\")\n",
    "    most_points_finder = re.compile(r\"Points\")\n",
    "    year_finder = re.compile(r\"201[0-9]\")\n",
    "    most_points = []\n",
    "\n",
    "    # Get the base page and find the URLs to all other relevant pages.\n",
    "    base_url=\"https://www.basketball-reference.com\"\n",
    "    base_page_source = requests.get(base_url).text\n",
    "    base_soup = BeautifulSoup(base_page_source, \"html.parser\")\n",
    "    link_tags = base_soup.find_all(name='option', value=True, string=link_finder)\n",
    "    pages = [base_url + tag.attrs[\"value\"] for tag in link_tags]\n",
    "    \n",
    "    # Crawl through the individual pages and record the data.\n",
    "    for page in pages:\n",
    "        time.sleep(1)               # PAUSE, then request the page.\n",
    "        soup = BeautifulSoup(requests.get(page).text, \"html.parser\")\n",
    "\n",
    "        #Find the tag with corresponding to the highest scorer\n",
    "        temp_tag = soup.find(name=\"caption\", string=most_points_finder)\n",
    "        temp_tag = temp_tag.next_sibling\n",
    "        temp_tag = temp_tag.next_sibling\n",
    "        \n",
    "        #Get the name, points, and year\n",
    "        name_tag = temp_tag.find('a').string\n",
    "        points_tag = int(temp_tag.find('td', class_= 'value').string)\n",
    "        year = int(re.findall(year_finder, page)[0])\n",
    "        most_points.append((year, str(name_tag), points_tag))\n",
    "        \n",
    "    return(most_points)\n",
    "    raise NotImplementedError(\"Problem 3 Incomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the list of tuples as a pickle file\n",
    "value = prob3()\n",
    "with open(\"ans3\", \"wb\") as fp:\n",
    "    pickle.dump(value,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "The website IMDB contains a variety of information on movies. Specifically,\n",
    "information on the top 10 box offce movies of the week can be found at `https://www.imdb.\n",
    "com/chart/boxoffice`. Using `BeautifulSoup`, `Selenium`, or both, return a list of the top 10\n",
    "movies of the week and order the list according to the total grossing of the movies, from most\n",
    "money to the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob4():\n",
    "    \"\"\"\n",
    "    Sort the Top 10 movies of the week by Total Grossing, taken from \n",
    "    https://www.imdb.com/chart/boxoffice?ref_=nv_ch_cht.\n",
    "\n",
    "    Returns:\n",
    "        titles (list): Top 10 movies of the week sorted by total grossing\n",
    "    \"\"\"\n",
    "    # Get the base page and find the URLs to all other relevant pages.\n",
    "    base_url=\"https://www.imdb.com/chart/boxoffice\"\n",
    "    base_page_source = requests.get(base_url).text\n",
    "    base_soup = BeautifulSoup(base_page_source, \"html.parser\")\n",
    "    movie_ranks = []\n",
    "    movie_money = []\n",
    "    \n",
    "    #Get all the titles and prices\n",
    "    movie_tags = base_soup.find_all(name='td', class_ = \"titleColumn\")\n",
    "    movie_gross = base_soup.find_all(name='span', class_ = 'secondaryInfo')\n",
    "    for tag in movie_tags:\n",
    "        movie_ranks.append(str(tag.a.string))\n",
    "    for tag in movie_gross:\n",
    "        movie_money.append(float(str(tag.string)[1:-1]))\n",
    "        \n",
    "    #Create a data frame for sorting all the data in the proper order\n",
    "    df = pd.DataFrame({\n",
    "        'col1': movie_ranks,\n",
    "        'col2': movie_money})\n",
    "    \n",
    "    #Return the data in descending order\n",
    "    df = df.sort_values(by=['col2'],ascending=False)\n",
    "    return(list(df['col1']))\n",
    "        \n",
    "    raise NotImplementedError(\"Problem 4 Incomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the list of movie names as a pickle file\n",
    "value = prob4()\n",
    "with open(\"ans4\", \"wb\") as fp:\n",
    "    pickle.dump(value,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "The arXiv (pronounced \"archive\") is an online repository of scientific publications,\n",
    "hosted by Cornell University. Write a function that accepts a string to serve as a search\n",
    "query defaulting to linkedin. Use `Selenium` to enter the query into the search bar of `https:\n",
    "//arxiv.org` and press Enter. The resulting page has up to 50 links to the PDFs of technical\n",
    "papers that match the query. Gather these URLs, then continue to the next page (if there are\n",
    "more results) and continue gathering links until obtaining at most 150 URLs. Return the list\n",
    "of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob5(search_query='linkedin'):\n",
    "    \"\"\"Use Selenium to enter the given search query into the search bar of\n",
    "    https://arxiv.org and press Enter. The resulting page has up to 25 links\n",
    "    to the PDFs of technical papers that match the query. Gather these URLs,\n",
    "    then continue to the next page (if there are more results) and continue\n",
    "    gathering links until obtaining at most 100 URLs. Return the list of URLs.\n",
    "\n",
    "    Returns:\n",
    "        (list): Up to 100 URLs that lead directly to PDFs on arXiv.\n",
    "    \"\"\"\n",
    "    #Create a list and initiate the browser\n",
    "    url_list = []\n",
    "    browser = webdriver.Chrome('/home/mark/chromedriver')\n",
    "    browser.get(\"https://arxiv.org\")\n",
    "    try:\n",
    "        #Try to use the search bar and search the given query\n",
    "        search_bar = browser.find_element_by_name('query')\n",
    "        search_bar.clear()\n",
    "        search_bar.send_keys(search_query)\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "        #Keep going from page to page until we have 150 names or there are no more pages\n",
    "        while(True):\n",
    "            page_soup = BeautifulSoup(browser.page_source,'html.parser')\n",
    "            tech_tags = page_soup.find_all(name='p', class_ = \"list-title is-inline-block\")\n",
    "            #Save all of the links found on the page\n",
    "            for tag in tech_tags:\n",
    "                try:\n",
    "                    url_list.append(str(tag.span.a['href']))\n",
    "                except: \n",
    "                    pass\n",
    "\n",
    "            #Return the list if it is already at least 150 long\n",
    "            if(len(url_list)>= 150):\n",
    "                browser.close()\n",
    "                return(url_list[:150])\n",
    "            try:\n",
    "                #Otherwise click the next button to the next page\n",
    "                next_bar = browser.find_element_by_class_name('pagination-next')\n",
    "                next_bar.click()\n",
    "            except:\n",
    "                #If there is no next page return the list\n",
    "                browser.close()\n",
    "                return(url_list)  \n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find the search bar!\")\n",
    "    raise NotImplementedError(\"Problem 5 Incomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the answer as a pickle file\n",
    "value = prob5()\n",
    "with open(\"ans5\", \"wb\") as fp:\n",
    "    pickle.dump(value,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
